import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import datetime

# Chargement du modÃ¨le arabe DZ
model_name = "akhooli/gpt2-small-arabic"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Fonction principale avec historique

def chat_fn(message, history):
    prompt = f"Ù…Ø³ØªØ®Ø¯Ù…: {message}\nØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:"
    result = generator(prompt, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    response = result[0]["generated_text"].split("Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:")[-1].strip()

    # Sauvegarde historique
    with open("historique.txt", "a", encoding="utf-8") as f:
        now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        f.write(f"[{now}] Utilisateur: {message}\nRÃ©ponse: {response}\n---\n")

    return response

# Interface Gradio avec logo
with gr.Blocks(theme="soft") as demo:
    gr.Image(value="logo.png", show_label=False, show_download_button=False, container=False, height=150)

    gr.ChatInterface(
        fn=chat_fn,
        title="DZGPT ğŸ‡©ğŸ‡¿ | Ø¯Ø±Ø¯Ø´ Ù…Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
        description="ØªØ­Ø¯Ø« Ù…Ø¹ Ø¨ÙˆØª Ø°ÙƒÙŠ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. ÙŠØ¯Ø¹Ù… Ø§Ù„ØµÙˆØª ÙˆØ§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…Ù†Ø·ÙˆÙ‚Ø©.",
        examples=["ÙƒÙŠÙØ§Ø´ Ø¯Ø§ÙŠØ±ØŸ", "Ø£Ø¹Ø·ÙŠÙ†ÙŠ Ù†ÙƒØªØ©", "ØªØ±Ø¬Ù…Ù„ÙŠ Ù„Ù„ÙØ±Ù†Ø³ÙŠØ©"],
        retry_btn="ğŸ” Ø¥Ø¹Ø§Ø¯Ø©",
        clear_btn="ğŸ§¹ Ù…Ø³Ø­",
        submit_btn="ğŸ—£ï¸ Ø£Ø±Ø³Ù„",
        input_audio="microphone",
        output_audio="auto",
        textbox=gr.Textbox(placeholder="Ø£ÙƒØªØ¨ Ù‡Ù†Ø§...", container=True, scale=7)
    )

demo.launch()
